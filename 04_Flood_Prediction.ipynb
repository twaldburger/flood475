{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNK7vwaVz3/mPByWgqpwas",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twaldburger/flood475/blob/master/04_Flood_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flood prediction\n",
        "In this last notebook, we apply our model trained in [03_Model_Training.ipynb](https://github.com/twaldburger/flood475/blob/master/03_Model_Training.ipynb) to new data it has not seen before.\n",
        "\n",
        "Please go through the explanations and code step-by-step and run each code cell.\n",
        "> **Task:** Task and questions are marked like this. Please try to answer them before proceeding with the next cell."
      ],
      "metadata": {
        "id": "1LwiXQ_bxXhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Preparations\n",
        "In this first section, we handle all imports and set some variables. We also initialize the connection to GEE."
      ],
      "metadata": {
        "id": "yyflEXvozRXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dependencies\n",
        "All dependencies required for this notebook are pre-installed in Google Colab. We can therefore just import them."
      ],
      "metadata": {
        "id": "9hJLEpXu_OTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "import geemap.colormaps as cm\n",
        "import google\n",
        "import joblib\n",
        "import numpy as np\n",
        "from osgeo import gdal\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "XoX_v2d5CLBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define global variables\n",
        "The cell below defines some global variables.\n",
        "- `PROJECT_ID` This is you Gee project ID. If you do not remember it, you can go to the [GEE code editor](https://code.earthengine.google.com/) and list your project by clicking on your user symbol in the top-right corner.\n",
        "- `MODEL_NAME` The name of your model you want to import."
      ],
      "metadata": {
        "id": "QiRFwVJZ_Zbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = ''    # @param {type: 'string'}\n",
        "MODEL_NAME = 'flood_prediction_1' # @param {type: 'string'}"
      ],
      "metadata": {
        "id": "_-VEJ-RuCK-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive\n",
        "We will use Google Drive to store our preliminary results from GEE because we can mount it to Google Colab and therefore easily write and read data without the need of manually down- and uploading datasets.\n",
        "\n",
        "**Important!** The cell below mounts your Google Drive to Google Colab and creates a new folder (named _geo475_ee_). This folder will be removed again at the end of the exercise (you can also keep it if you want, of course). **To make sure that we are not deleting any of your personal data, do not change the `data_dir`-variable in the cell below unless you know what you are doing.**"
      ],
      "metadata": {
        "id": "s5xmBu22_wLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = Path('/content/gdrive/MyDrive/geo475_ee')\n",
        "\n",
        "## mount Google Drive to Colab\n",
        "if not data_dir.parent.exists():\n",
        "  google.colab.drive.mount('/content/gdrive')\n",
        "\n",
        "## create output directory for the project\n",
        "if not data_dir.exists():\n",
        "  data_dir.mkdir()"
      ],
      "metadata": {
        "id": "U6vVhhhICK8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Google Earth Engine\n",
        "In the cell below, we connect to GEE using the same apporoach shown in [01_Connecting_to_GEE.ipynb](https://github.com/twaldburger/flood475/blob/master/01_Connecting_to_GEE.ipynb)."
      ],
      "metadata": {
        "id": "MFyjMxDLAGWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab.auth.authenticate_user()\n",
        "credentials, project_id = google.auth.default()\n",
        "ee.Initialize(credentials, project=PROJECT_ID)\n",
        "print(ee.String('Nice! That worked! :-)').getInfo())"
      ],
      "metadata": {
        "id": "KPwhZlBrCK5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define GEE data sources\n",
        "We define the same datasets we used to create our training dataset in [02_Creating_Training_Data.ipynb](https://github.com/twaldburger/flood475/blob/master/02_Creating_Training_Data.ipynb) since we want to extract the same information to feed the model. Note that we no longer require [Global Flood Database v1 (2000-2018)](https://developers.google.com/earth-engine/datasets/catalog/GLOBAL_FLOOD_DB_MODIS_EVENTS_V1#description) since this dataset was only used to define our target variable in model training."
      ],
      "metadata": {
        "id": "YE5QpeVCASsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flood_ds = ee.ImageCollection('GLOBAL_FLOOD_DB/MODIS_EVENTS/V1')\n",
        "elevation_ds = ee.ImageCollection('COPERNICUS/DEM/GLO30')\n",
        "landcover_ds = ee.ImageCollection(\"ESA/WorldCover/v200\")\n",
        "precipitation_ds = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')\n",
        "flowaccumulation = ee.Image(\"MERIT/Hydro/v1_0_1\").select('upa')"
      ],
      "metadata": {
        "id": "xkSmK_o3CK2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define GEE functions\n",
        "In contrast to our approach when creating the training data, we do not want to retrieve GEE data for individual locations but we want to download data for an area. Since we will do this for multiple datasets, it makes sense to define a function. This function not only downloads data but it also clips and reprojects each dataset to match their bounds and spatial resolution.\n",
        "> **Task**: Take a look at the code and try to unterstand what is happening. Do you see room for improvement?"
      ],
      "metadata": {
        "id": "3D4vBT_VBjDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_gee(name, img, aoi, scale, crs='EPSG:4326', clipped_dem=None):\n",
        "    \"\"\"\n",
        "    Clip and rescale a GEE dataset.\n",
        "    Then extract the result as numpy array along with some metadata.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    name : str\n",
        "        Name of the dataset. This is not the GEE dataset id but just a name for\n",
        "        your reference.\n",
        "    img : ee.image.Image or ee.imagecollection.ImageCollection\n",
        "        Image or ImageCollection from which to clip.\n",
        "    aoi : ee.geometry.Geometry\n",
        "        Are of interest to clip ``img`` to.\n",
        "    scale : float\n",
        "        Pixel resolution of the output image.\n",
        "    crs : str, optional\n",
        "        Coordinate reference system of the output image.\n",
        "    clipped_dem : ee.image.Image, optional\n",
        "        Clipped and reprojected DEM. This is used to compute aspect and slope.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary containing the ee.Image, numpy array and metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"extracting {name}\", end='... ')\n",
        "\n",
        "    try:\n",
        "\n",
        "        ## clip and rescale the image\n",
        "        if name=='slope':\n",
        "            image = ee.Terrain.slope(clipped_dem)\n",
        "        elif name=='aspect':\n",
        "            image = ee.Terrain.aspect(clipped_dem)\n",
        "        else:\n",
        "          if isinstance(img, ee.image.Image):\n",
        "              image = img.clip(aoi).reproject(crs=crs, scale=scale)\n",
        "          elif isinstance(img, ee.imagecollection.ImageCollection):\n",
        "              image = img.filterBounds(aoi).mosaic().clip(aoi).reproject(crs=crs, scale=scale)\n",
        "          else:\n",
        "              raise TypeError(f\"Unknown format ({type(img).__name__}).\")\n",
        "\n",
        "        ## create a metadata dictionary\n",
        "        dct = {\n",
        "            'name':  name,\n",
        "            'image': image,\n",
        "            'scale': image.projection().nominalScale().getInfo(),\n",
        "        }\n",
        "        minmax = image.reduceRegion(reducer=ee.Reducer.minMax(), geometry=aoi, scale=scale).getInfo()\n",
        "        try:\n",
        "            dct['min'] = min(minmax.values())\n",
        "            dct['max'] = max(minmax.values())\n",
        "            dct['vis'] = {'min': dct['min'], 'max': dct['max'], 'palette': cm.palettes.viridis}\n",
        "        except TypeError: # is raised if min or max is None\n",
        "            pass\n",
        "\n",
        "        ## download image as numpy array\n",
        "        arr = geemap.ee_to_numpy(image, default_value=80)\n",
        "\n",
        "        ## normalize elevation\n",
        "        if name=='elevation':\n",
        "            arr = (arr-arr.min()) / (arr.max()-arr.min())\n",
        "\n",
        "        dct['array'] = arr\n",
        "        print('done!')\n",
        "        return {name: dct}\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        raise RuntimeError(f\"Caught exception in dataset {name}\") from e"
      ],
      "metadata": {
        "id": "8E6bS8f4CGis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the trained model\n",
        "We now import our model trained in [03_Model_Training.ipynb](https://github.com/twaldburger/flood475/blob/master/03_Model_Training.ipynb) from Google Drive."
      ],
      "metadata": {
        "id": "JuH2FzLPBNC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load(data_dir/f\"{MODEL_NAME}.pkl\")"
      ],
      "metadata": {
        "id": "CHPcCedGCKz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Create flood probability map"
      ],
      "metadata": {
        "id": "clIDJFwaEetO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the region of interest\n",
        "We use an interactive map to define for which region we want to create a flood probability map.\n",
        "> **Task:** Draw a region of interest (ROI) using the _Draw a rectangle_-tool from the tool bar on the left.\n"
      ],
      "metadata": {
        "id": "1tDdJyADD2PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "Map"
      ],
      "metadata": {
        "id": "JKrSdmz9CKxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract features from GEE\n",
        "We now use our feature extraction function defined above to get all relevant data for the ROI."
      ],
      "metadata": {
        "id": "Z6sXOg1HEiWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## define spatial target resolution\n",
        "scale = elevation_ds.select('DEM').first().projection().nominalScale().getInfo()\n",
        "\n",
        "## make sure the roi is not too large\n",
        "roi = ee.FeatureCollection(Map.draw_features)\n",
        "n = elevation_ds.select('DEM').mosaic().reduceRegion(reducer=ee.Reducer.count(), geometry=roi.geometry(), scale=scale).getInfo()['DEM']\n",
        "if n > 250000:\n",
        "  Map.remove_drawn_features()\n",
        "  raise ValueError(f\"ROI is too large ({n} pixels). Maximum number of pixels allowed is 250000.\")\n",
        "\n",
        "## extract all relevant features\n",
        "data = {}\n",
        "data.update(extract_features_from_gee('elevation', elevation_ds.select('DEM'), roi.geometry(), scale))\n",
        "data.update(extract_features_from_gee('slope', _, roi.geometry(), scale, clipped_dem=data['elevation']['image']))\n",
        "data.update(extract_features_from_gee('aspect', _, roi.geometry(), scale, clipped_dem=data['elevation']['image']))\n",
        "data.update(extract_features_from_gee('landcover', landcover_ds.first(), roi.geometry(), scale))\n",
        "data.update(extract_features_from_gee('upstream_drainage_area', flowaccumulation, roi.geometry(), scale))\n",
        "\n",
        "## precipitation is different since we need to aggregate to the mean daily max precipitation per year\n",
        "lst = []\n",
        "for year in range(2004, 2023):\n",
        "  daily_max = precipitation_ds.select('precipitation').filterBounds(roi.geometry()).filter(ee.Filter.date(f\"{year}-01-01\", f\"{year+1}-01-01\")).max()\n",
        "  lst.append(daily_max)\n",
        "daily_max_mean = ee.ImageCollection(lst).mean()\n",
        "data.update(extract_features_from_gee('daily_max_precipitation', daily_max_mean, roi.geometry(), scale))"
      ],
      "metadata": {
        "id": "Bj-CrnMyCKuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the model\n",
        "We have extracted multiple 2-dimensional datasets but the model expects a single matrix-like input. We therefore need to reshape our data before using it as model input. Since we are working with numpy arrays, it is important that we ensure the same order of features we used to train the model.\n",
        "\n",
        "We then run the model and reshape the results back to the original dimensions of our ROI."
      ],
      "metadata": {
        "id": "x92d4GaAIqof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## reshape 2d arrays to matrix\n",
        "model_input = np.array([data[name]['array'].flatten() for name in model.feature_names_in_]).T\n",
        "\n",
        "## run the model\n",
        "probability = model.predict_proba(model_input)\n",
        "\n",
        "## reshape model output to the roi dimensions\n",
        "probability = probability[:, 1].reshape(data['elevation']['array'].shape)"
      ],
      "metadata": {
        "id": "Gf-CJH-ECKjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the results\n",
        "In order to visualize our flood probability map with the _geemap_-library, we need to create an _ee.Image_-object from our numpy array. This requires to define a geotransformation so the image can be displayed correctly on a map. The geotransformation of our probability layer is identical to the transoformation of the input data. However, since I have not found an elegant way to extract the geotransformation from a GEE image, I am using a hack where I save the image as GeoTif and then read the transformation using gdal."
      ],
      "metadata": {
        "id": "RgSTJlrv5FRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## write the probability image to a geotif file\n",
        "geemap.ee_export_image(data['elevation']['image'], data_dir/f\"dem.tif\")\n",
        "\n",
        "## read the geotransformation using gdal\n",
        "ds = gdal.Open(str(data_dir/'dem.tif'))\n",
        "gtf = ds.GetGeoTransform()\n",
        "del ds\n",
        "\n",
        "## reorder elements to match GEE's order\n",
        "affine = [gtf[1], gtf[2], gtf[0], gtf[4], gtf[5], gtf[3]]\n",
        "\n",
        "## create ee.image and metadata for flood probability\n",
        "image = geemap.numpy_to_ee(np.squeeze(probability).T, crs='EPSG:4326', transform=affine, band_names='proba')\n",
        "dct = {\n",
        "    'name': 'flood_probability',\n",
        "    'image': image,\n",
        "    'scale': scale,\n",
        "    'min': probability.min(),\n",
        "    'max': probability.max(),\n",
        "    'vis': {'min': probability.min(), 'max': probability.max(), 'palette': cm.palettes.viridis}\n",
        "}\n",
        "data['flood_probability'] = dct"
      ],
      "metadata": {
        "id": "dl1SGaAtGFqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create our final map where we show all input layers as well as the flood probability layer.\n",
        "> **Task:** Interpret your flood probability map:\n",
        "1. Does the map make sense?\n",
        "2. Can you see which features influences the map the most? Is your feeling in accordance with the feature importance plot for your model?\n",
        "3. Can you spot errors or artifacts? If yes, how can they be explained?\n",
        "4. Rerun this notebook and look at different ROI."
      ],
      "metadata": {
        "id": "H-7dtPZWP7km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "for name in data.keys():\n",
        "  if name=='flood_probability':\n",
        "    Map.add_layer(data[name]['image'], data[name]['vis'], name=name, shown=True)\n",
        "  else:\n",
        "    Map.add_layer(data[name]['image'], data[name]['vis'], name=name, shown=False)\n",
        "Map.addLayerControl()\n",
        "Map.centerObject(roi)\n",
        "Map"
      ],
      "metadata": {
        "id": "2akJcrVK-6PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Clean up Google Drive\n",
        "This was the last notebook of this exercise. Running the cell below removes the project files we created on your Google Drive and unmounts Google Drive from Colab."
      ],
      "metadata": {
        "id": "lr3MKjqNCHvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi6RhXo5CFTU"
      },
      "outputs": [],
      "source": [
        "## remove temporary jupyter checkpoints from Google Drive\n",
        "checkpoints = Path(data_dir/'.ipynb_checkpoints')\n",
        "if checkpoints.exists():\n",
        "  for f in checkpoints.glob('*'):\n",
        "    f.unlink()\n",
        "  checkpoints.rmdir()\n",
        "\n",
        "## remove all project files from Google Drive\n",
        "for f in data_dir.glob('*'):\n",
        "  f.unlink()\n",
        "data_dir.rmdir()\n",
        "\n",
        "## unmount Google Drive\n",
        "google.colab.drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Feedback\n",
        "This is the last part of the exercise and I hope that it was helpful and fun. Since I have done this for the first time, I would much appreciate if you could take 2 minutes to provide me a short feedback.\n",
        "\n",
        "Please run the cell below to display a Google Form where you can provide your feedback. I will not collect your mail address so the feedback is anonymous."
      ],
      "metadata": {
        "id": "Qn2KJ54YDQeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://docs.google.com/forms/d/e/1FAIpQLScg8j6ORkqgWw4QEHpkeOy2PxYKSdgop3PPvaA1_WT54igFIA/viewform?embedded=true\" width=\"640\" height=\"1376\" frameborder=\"0\" marginheight=\"0\" marginwidth=\"0\">Wird geladenâ€¦</iframe>"
      ],
      "metadata": {
        "id": "Hq-v6_umCtMr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}