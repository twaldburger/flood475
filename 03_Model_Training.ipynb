{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SGUFRpVSwOo",
        "tags": []
      },
      "source": [
        "# Training a flood prediction model\n",
        "In this second notebook, we use the training dataset created in [02_Creating_Training_Data.ipynb](https://github.com/twaldburger/flood475/blob/master/02_Creating_Training_Data.ipynb) to train a flood prediction model\n",
        "> **Task:** Tasks and questions are marked like this. Please try to answer them before proceeding with the next cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "2eovmY2v8QX-"
      },
      "source": [
        "---\n",
        "## Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAw0i2G-8QX-"
      },
      "source": [
        "### Install additional dependencies\n",
        "Not all our required libraries are pre-installed in Colab. We therefore install additional libraries using [pip](https://pip.pypa.io/en/stable/). After installation, we have to restart our runtime before we can import the library. Please make sure that you click the _RESTART SESSION_-button which is displayed at the end of the code cell's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV-a7YlM8QX_"
      },
      "outputs": [],
      "source": [
        "! pip install pycaret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZODLMLcFTmb9"
      },
      "source": [
        "### Import dependencies\n",
        "We can now import all required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feYz5o1vTy6a"
      },
      "outputs": [],
      "source": [
        "import google\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from geemap.legends import builtin_legends\n",
        "from pathlib import Path\n",
        "from pycaret.classification import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53tMbUIhUA6S"
      },
      "source": [
        "### Define global variables\n",
        "The cell below defines some global variables.\n",
        "- `TRAINING_DATA` This is the name of the dataset we created in [02_Creating_Training_Data.ipynb](https://github.com/twaldburger/flood475/blob/master/02_Creating_Training_Data.ipynb). You do not have to specify the path but only the file name which is most likely _roi_sample_.\n",
        "- `MODEL_NAME` The name of your model. This becomes relevant if you want to save multiple different models.\n",
        "- `SEED` This is the seed value used for splitting our training data into a train and a test dataset. Setting this value makes our results reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqiNHbFcTy3n"
      },
      "outputs": [],
      "source": [
        "TRAINING_DATA = 'roi_sample' # @param {type: 'string'}\n",
        "MODEL_NAME = 'flood_prediction_1' # @param {type: 'string'}\n",
        "SEED = 123 # @param {type: 'integer'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQiaIA4mU3SD"
      },
      "source": [
        "### Mount Google Drive\n",
        "We will use Google Drive to store our preliminary results from GEE because we can mount it to Google Colab and therefore easily write and read data without the need of manually down- and uploading datasets.\n",
        "\n",
        "**Important!** The cell below mounts your Google Drive to Google Colab and creates a new folder (named _geo475_ee_). This folder will be removed again at the end of the exercise (you can also keep it if you want, of course). **To make sure that we are not deleting any of your personal data, do not change the `data_dir`-variable in the cell below unless you know what you are doing.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN4A0DaDTy1D"
      },
      "outputs": [],
      "source": [
        "data_dir = Path('/content/gdrive/MyDrive/geo475_ee')\n",
        "\n",
        "## mount Google Drive to Colab\n",
        "if not data_dir.parent.exists():\n",
        "  google.colab.drive.mount('/content/gdrive')\n",
        "\n",
        "## create output directory for the project\n",
        "if not data_dir.exists():\n",
        "  data_dir.mkdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m-bcYk4VoV-"
      },
      "source": [
        "---\n",
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6R_BwkzVuir"
      },
      "source": [
        "### Data import\n",
        "First, we import our training dataset from Google Drive and remove unnecessary columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywlU3rbMTyya"
      },
      "outputs": [],
      "source": [
        "## import from google drive\n",
        "df = pd.read_csv(data_dir/f\"{TRAINING_DATA}_norm.csv\")\n",
        "\n",
        "## make target variable binary\n",
        "df['flooded'] = 0\n",
        "df.loc[df['floods']>0, 'flooded'] = 1\n",
        "\n",
        "## remove unnecessary columns\n",
        "df.drop(['floods', 'system:index', 'first', '.geo'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4aDF8iQYzJe"
      },
      "source": [
        "### Exploratory data analysis\n",
        "We now have a first look at the columns in the dataframe.\n",
        "> **Task:** Look at the descriptive statistics and see if you spot anything unexpected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htt3N63JTyv1"
      },
      "outputs": [],
      "source": [
        "## get descriptive statistics of all columns\n",
        "df.describe(include='all').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YgY9PqZiim4"
      },
      "source": [
        "Let's check how flood frequency is distributed and how it relates to landcover by creating a stacked barplot showing the flood frequency counts per landcover class.\n",
        "> **Task:** Try to answer the following questions:\n",
        "1. What are your key observations when looking at the barplot?\n",
        "2. Does the plot make sense? Do you see anything unexpected?\n",
        "3. Do you see signs of biased or wrong training data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc2dmYoQe9J2"
      },
      "outputs": [],
      "source": [
        "## create a mapper to assign landcover class names\n",
        "mapper = {}\n",
        "for cls in builtin_legends['ESA_WorldCover']:\n",
        "  k, v = cls.split(' ', 1)\n",
        "  mapper[float(k)] = v\n",
        "df['landcover_names'] = df['landcover'].map(mapper)\n",
        "\n",
        "## stacked barplot showing flood frequency counts per landcover class\n",
        "df_plot = df.groupby(['landcover_names', 'flooded']).size().reset_index()\n",
        "df_plot = df_plot.pivot(columns='landcover_names', index='flooded', values=0)\n",
        "df.drop(['landcover_names'], axis=1, inplace=True)\n",
        "df_plot.plot(kind='bar', stacked=True, figsize=(10, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV2KuUFDqTwK"
      },
      "source": [
        "> **Task:** Which other training variables are you interested in exploring? Try to create a few more plots to get an idea of your training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVLdv2Kezouo"
      },
      "source": [
        "Next, let's have a look at correlations with our target variable.\n",
        "> **Task:** Do you see any strong correlations? How could we use a high correlation to our benefit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekL5b7osTylu"
      },
      "outputs": [],
      "source": [
        "corr = df.corr(method='pearson')\n",
        "corr['flooded'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tTbohtN0wX6"
      },
      "source": [
        "And also at the correlations between all other variables.\n",
        "> **Task:** Try to answer the following questions:\n",
        "1. Between which variables do you see (strong) correlation?\n",
        "2. To what extent does it make sense to look at the correlation? Where do you see potential problems?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOCIF-3RTyjX"
      },
      "outputs": [],
      "source": [
        "# quantify correlations between all variables\n",
        "plt.figure(figsize=(13, 8))\n",
        "sns.set(font_scale=0.6)\n",
        "sns.heatmap(corr, cmap='RdYlGn', annot=True, center=0, fmt=\".2g\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcyuahr13Qih"
      },
      "source": [
        "### Feature engineering\n",
        "We saw that _daily_max_precipitation_ is strongly correlated in many columns. We therefore create a new variable by aggregating all _daily_max_precipitation_ using the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX2Vd9qFTygt"
      },
      "outputs": [],
      "source": [
        "cols = [c for c in df.columns if c.startswith('daily_max')]\n",
        "df['daily_max_precipitation'] = df[cols].mean(axis=1)\n",
        "df.drop(cols, axis=1, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HQIkhnd7Zl2"
      },
      "source": [
        "---\n",
        "## Model training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeIje1hJy3_X"
      },
      "source": [
        "### Setup\n",
        "With this function, we initialize the training environment. We are using the absolute minimum for the setup by only defining the required parameters. However, the customization options are almost endless.\n",
        "> **Task:** Come back to this cell after your initial run and try if you can get better results by adjusting your training environment. You can find the documentation for the _setup_-method [here](https://pycaret.readthedocs.io/en/latest/api/classification.html#pycaret.classification.ClassificationExperiment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0sGIzkNTyWz"
      },
      "outputs": [],
      "source": [
        "classifier = setup(data=df, target='flooded', session_id=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jquzkrhxz_8N"
      },
      "source": [
        "### Compare models\n",
        "We now train and evaluate multiple models. The visualisation of this function is a scoring grid with average cross-validated scores. The output stored in `best` is the highest scoring model. Here, we want to focus on the following metrics:\n",
        "\n",
        "- _Accuracy:_ How often is the model right?\n",
        "- _Recall:_ How many positive predictions can the model identify?\n",
        "- _Precision:_ How often are positive predictions correct?\n",
        "- _F1-Score:_ Combination of recall and precision. A high F1-score signifies that the model can effectively identify positive cases while minimizing false positives and false negatives.\n",
        "\n",
        "> **Task:** Conduct a short internet research to learn a little bit about the type of model which scored best for your training data. Then, try to answer the following questions:\n",
        "1. How high is your best model's accuracy? Do you think this is a good performance?\n",
        "2. When looking at the other metrics, would you argue that another model actually performs better than the one with the highest accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3CtpjzpTyRz"
      },
      "outputs": [],
      "source": [
        "best = compare_models(sort='F1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbiO-_av0whg"
      },
      "source": [
        "### Analyze best model\n",
        "We now focus on the model with the highest F1-score and plot the confusion matrix by running the cell below. Below is a quick refresher on the 4 fields of the confusion matrix but it is also worth to have a look at [this page](https://www.v7labs.com/blog/confusion-matrix-guide) which shows how the metrics from above link to the confusion matrix.\n",
        "\n",
        "- _True Positive (TP):_ a class is predicted true and is true in reality (locations that are flooded and are predicted as flooded)\n",
        "- _True Negative (TN):_ a class is predicted false and is false in reality (locations that not flooded and are predicted as not flooded)\n",
        "- _False Positive (FP):_ a class is predicted true but is false in reality (locations that are not flooded but are predicted as flooded)\n",
        "- _False Negative (FN):_ a class is predicted false but is true in reality (locations that are flooded but predicted as not flooded)\n",
        "\n",
        "> **Task:** Try to answer the following questions:\n",
        "1. What can you read from the confusion matrix of your model?\n",
        "2. Does your model perform better when correctly predicting flooded locatations or does it perform better when correctly predicting non-flooded locations.\n",
        "3. Thinking about risk and insurance, which of the 4 fields do think is most important?\n",
        "4. Thinking about risk and insurance, what does a high number of false negatives mean?\n",
        "5. Thinking about risk and insurance, what does a high number of false positives mean?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfCT-IV8TyPY"
      },
      "outputs": [],
      "source": [
        "plot_model(best, plot='confusion_matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNXyxMhA-hoz"
      },
      "source": [
        "We will also have a look at the importance of our input variables.\n",
        "> **Task:** Try to answer the following questions:\n",
        "1. Which features are more important and which are less important?\n",
        "2. How could we use this information to improve our pipeline?\n",
        "3. How could the selection of our training data (and our region of interest) influence feature importance?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hzaKK35TyMw"
      },
      "outputs": [],
      "source": [
        "plot_model(best, plot='feature')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E3wgDXf2y7E"
      },
      "source": [
        "### Save model\n",
        "We finalize the model by training it again on the full dataset - including the 30% used for validation. This does not change any parameter of the model but only refits on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPQ-P3IX2BY-"
      },
      "outputs": [],
      "source": [
        "finalize_model(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw7IQF_f3jd7"
      },
      "source": [
        "We finally save the model to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaD6G8Yl3CO7"
      },
      "outputs": [],
      "source": [
        "save_model(best, model_name=data_dir/MODEL_NAME, model_only=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}