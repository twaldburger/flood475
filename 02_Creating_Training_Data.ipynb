{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87ySFUlm0bQj"
   },
   "source": [
    "# Creating a training dataset\n",
    "In this first notebook, we create a set of training locations which we can then use to train a flood prediction model.\n",
    "\n",
    "Please go through the explanations and code step-by-step and run each code cell.\n",
    "> **Task:** Task and questions are marked like this. Please try to answer them before proceeding with the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MH1oy-GBxDu"
   },
   "source": [
    "---\n",
    "## Preparations\n",
    "In this first section, we handle all imports and set some variables. We also initialize the connection to GEE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0R9h73AZB23r"
   },
   "source": [
    "### Import dependencies\n",
    "All dependencies required for this notebook are pre-installed in Google Colab. We can therefore just import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kx4oC-Q069m5"
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import geemap.colormaps as cm\n",
    "import google\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxK350kN4lYj"
   },
   "source": [
    "### Define global variables\n",
    "The cell below defines some global variables.\n",
    "- `PROJECT_ID` This is you Gee project ID. If you do not remember it, you can go to the [GEE code editor](https://code.earthengine.google.com/) and list your project by clicking on your user symbol in the top-right corner.\n",
    "- `SAMPLE_SIZE` This is the number of training locations we want to create. To keep computation times small, I suggest that you choose a sample size below 5000.\n",
    "- `SEED` This is the seed value used for random sampling. Setting this here makes our sampling reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWMQwXpI4kxw"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = ''    # @param {type: 'string'}\n",
    "SAMPLE_SIZE = 3000 # @param {type: 'integer'}\n",
    "SEED = 123         # @param {type: 'integer'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yg5bGjW7HW-H"
   },
   "source": [
    "### Mount Google Drive\n",
    "We will use Google Drive to store our preliminary results from GEE because we can mount it to Google Colab and therefore easily write and read data without the need of manually down- and uploading datasets.\n",
    "\n",
    "**Important!** The cell below mounts your Google Drive to Google Colab and creates a new folder (named _geo475_ee_). This folder will be removed again at the end of the exercise (you can also keep it if you want, of course). **To make sure that we are not deleting any of your personal data, do not change the `data_dir`-variable in the cell below unless you know what you are doing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6KMJWoGHXZb"
   },
   "outputs": [],
   "source": [
    "data_dir = Path('/content/gdrive/MyDrive/geo475_ee')\n",
    "\n",
    "## mount Google Drive to Colab\n",
    "if not data_dir.parent.exists():\n",
    "  google.colab.drive.mount('/content/gdrive')\n",
    "\n",
    "## create output directory for the project\n",
    "if not data_dir.exists():\n",
    "  data_dir.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiHrA3i04P7j"
   },
   "source": [
    "### Initialize Google Earth Engine\n",
    "In the cell below, we connect to GEE using the same apporoach shown in [01_Connecting_to_GEE.ipynb](https://github.com/twaldburger/flood475/blob/master/01_Connecting_to_GEE.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUga2F8h4eAg"
   },
   "outputs": [],
   "source": [
    "google.colab.auth.authenticate_user()\n",
    "credentials, project_id = google.auth.default()\n",
    "ee.Initialize(credentials, project=PROJECT_ID)\n",
    "print(ee.String('Nice! That worked! :-)').getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXMLzsQtCYgM"
   },
   "source": [
    "---\n",
    "## Data\n",
    "In this section, we have a quick look at the datasets we will use to generate our training dataset. We use only data from the GEE Catalog for which you can find the links below:\n",
    "\n",
    "- [Global Flood Database v1 (2000-2018)](https://developers.google.com/earth-engine/datasets/catalog/GLOBAL_FLOOD_DB_MODIS_EVENTS_V1#description)\n",
    "- [CHIRPS Daily: Climate Hazards Group InfraRed Precipitation With Station Data (Version 2.0 Final)](https://developers.google.com/earth-engine/datasets/catalog/UCSB-CHG_CHIRPS_DAILY#description)\n",
    "- [Copernicus DEM GLO-30: Global 30m Digital Elevation Model](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_DEM_GLO30#description)\n",
    "- [MERIT Hydro: Global Hydrography Datasets](https://developers.google.com/earth-engine/datasets/catalog/MERIT_Hydro_v1_0_1#bands)\n",
    "- [ESA WorldCover 10m v200](https://developers.google.com/earth-engine/datasets/catalog/ESA_WorldCover_v200?hl=en#description)\n",
    "\n",
    "> **Task:** Familiarize yourself by following the links to the GEE Catalog. Try to answer the following questions:\n",
    "1. Which datasets are raster datasets? Which are vector?\n",
    "2. How do the data differ in (spatial) resolution?\n",
    "3. How many flood events does the [Global Flood Database v1 (2000-2018)](https://developers.google.com/earth-engine/datasets/catalog/GLOBAL_FLOOD_DB_MODIS_EVENTS_V1#description) contain?\n",
    "4. How many bands does the Digital Elevation Model (DEM) have?\n",
    "5. How is the precipitation dataset different from the others?\n",
    "6. How do these data limit us in training a flood prediction model? Try to look at the temporat and spatial extent covered by the different datasets. Hint: You can visualize a single day of CHIRPS precipitation data by running the next two code cells.\n",
    "\n",
    "With the code below, we import the relevant datasets from GEE. We are not reading the actual data into our memory (it would never fit) but we reference the datasets as a variables so we can start using it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6ESDkffKss6"
   },
   "source": [
    "### Import from GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8PKn32K8Oi1"
   },
   "outputs": [],
   "source": [
    "flood_ds = ee.ImageCollection('GLOBAL_FLOOD_DB/MODIS_EVENTS/V1')\n",
    "elevation_ds = ee.ImageCollection('COPERNICUS/DEM/GLO30')\n",
    "landcover_ds = ee.ImageCollection(\"ESA/WorldCover/v200\")\n",
    "precipitation_ds = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')\n",
    "flowaccumulation = ee.Image(\"MERIT/Hydro/v1_0_1\").select('upa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqX-slZ4FmiH"
   },
   "source": [
    "### Explore interactively\n",
    "This code generates an interactive map of the precipitation data we are using. Note that although the dataset contains daily precipitation of over 30 years, we are only visualizing a single day.\n",
    "> **Task:** Try to visualize other datasets as well. Hint: Many datasets define the code for a simple visualsation in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMAeqJXQ8FCr"
   },
   "outputs": [],
   "source": [
    "## add a temporal filter to the CHIRPS dataset and select the precipitation layer\n",
    "precipitation = precipitation_ds.filterDate('2018-05-01').select('precipitation')\n",
    "\n",
    "## define a color palette for nicer visualisation\n",
    "precipitation_vis = {\n",
    "    'min': 1,\n",
    "    'max': 17,\n",
    "    'palette': ['001137', '0aab1e', 'e7eb05', 'ff4a2d', 'e90000'],\n",
    "}\n",
    "\n",
    "## create and display an interactive map\n",
    "Map = geemap.Map()\n",
    "Map.add_layer(precipitation, precipitation_vis, 'Precipitation')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoI4EKW1Pmbh"
   },
   "source": [
    "---\n",
    "## Defining the training locations\n",
    "In this section, we sample the locations we want to use for training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQEyXWRuK8Ue"
   },
   "source": [
    "### Defining the region of interest\n",
    "First, we define our region of interest.\n",
    "> **Task:** Run the cell below to visualize all historic flood events in our dataset. Decide which area you want to use to train your model and draw a region of interest (ROI) using the _Draw a rectangle_-tool from the tool bar on the left. Keep in mind the spatial constraints of our other datasets when choosing the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtKL4FeUE76-"
   },
   "outputs": [],
   "source": [
    "## subtract the permanent water bodies from the flooded areas\n",
    "def mapper(img):\n",
    "  flood = img.select('flooded')\n",
    "  perm = img.select('jrc_perm_water')\n",
    "  return flood.multiply(perm.eq(0))\n",
    "all_floods = flood_ds.map(mapper).sum()\n",
    "\n",
    "## color flooded areas in blue\n",
    "flood_vis = {\n",
    "    'min': 0,\n",
    "    'max': 7,\n",
    "    'palette': cm.palettes.Blues\n",
    "    }\n",
    "\n",
    "## create and display an interactive map\n",
    "Map = geemap.Map()\n",
    "Map.add_layer(all_floods.selfMask(), flood_vis, 'Historic floods')\n",
    "Map.add_colorbar(flood_vis, label=\"Number of floods\", layer_name=\"Historic floods\", font_size=9)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2lSQyE3MNni"
   },
   "source": [
    "### Sampling random locations\n",
    "We use GEE to randomly sample locations within our ROI. The ROI is taken from the Map-object we defined in the cell above.\n",
    "> **Task:** What needs to be considered if we want to sample evenly distributed locations? Hint: it also mentioned in the documentation of the function we use: [ee.FeatureCollection.randomPoints](https://developers.google.com/earth-engine/apidocs/ee-featurecollection-randompoints).\n",
    "\n",
    "Run the cell below to randomly sample training locations. Note that we use the two variables `SAMPLE_SIZE` and `SEED` from the beginning of the notebook as input for the sampling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxQeQR7U_MAd"
   },
   "outputs": [],
   "source": [
    "## get the region of interest drawn on the map\n",
    "roi = ee.FeatureCollection(Map.draw_features)\n",
    "\n",
    "## randomly sample points within the ROI\n",
    "roi_sample = ee.FeatureCollection.randomPoints(\n",
    "    region=roi.geometry(),\n",
    "    points=SAMPLE_SIZE,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lNwFlEYTC3m"
   },
   "source": [
    "---\n",
    "## Data enrichment\n",
    "In this section, we will enrich our training locations uning several different datasets. The approach is always the same, namely performing a simple point lookup on each dataset using the locations we sampled above.\n",
    "As the point lookup will be repeated several times, we define a custom function which we then just can apply to each dataset.\n",
    "\n",
    "It is important to know that calling this function does not yet trigger any computations on GEE. This will only happen once we create and start a task for it (which we will do further down below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRP500WJ3xlB"
   },
   "outputs": [],
   "source": [
    "def singleband_point_query(fc, img, scale, prop):\n",
    "  \"\"\"\n",
    "  Run a point lookup on a single image.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  fc : ee.FeatureCollection\n",
    "    FeatureCollection of points for which to preform the lookup.\n",
    "  img : ee.Image\n",
    "    Image on which to perform the lookup.\n",
    "  scale : float\n",
    "    Spatial resolution of the image.\n",
    "  prop : str\n",
    "    Name of the property under which lookup values shall be added to the feature collection.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  ee.FeatureCollection\n",
    "    Input FeatureCollection with lookup values added as new property.\n",
    "  \"\"\"\n",
    "\n",
    "  ## extract the pixel values at each location\n",
    "  fc = img.reduceRegions(\n",
    "      collection=fc,\n",
    "      reducer=ee.Reducer.first(),\n",
    "      scale=scale\n",
    "  )\n",
    "\n",
    "  ## add the pixel values as a new feature property\n",
    "  def mapper(feature):\n",
    "    return feature.set(prop, feature.get('first'))\n",
    "  fc = fc.map(mapper)\n",
    "\n",
    "  return fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2VUtGXbTe5k"
   },
   "source": [
    "### Elevation, slope and aspect\n",
    "We start by enriching our training locations with information from the DEM. We not only use the elevation above sea level but also slope and aspect which we derive from the DEM using GEE's powerful built-in functions. Note how we are making use of the function we defined in the cell above.\n",
    "\n",
    "Dataset: [Copernicus DEM GLO-30: Global 30m Digital Elevation Model](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_DEM_GLO30#description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6j_p5N7zyNBk"
   },
   "outputs": [],
   "source": [
    "## elevation\n",
    "dem = elevation_ds.select('DEM')\n",
    "roi_sample = singleband_point_query(\n",
    "    fc=roi_sample,\n",
    "    img=dem.mosaic(),\n",
    "    scale=dem.first().projection().nominalScale().getInfo(),\n",
    "    prop='elevation'\n",
    ")\n",
    "\n",
    "## slope\n",
    "slope = ee.Terrain.slope(dem.mosaic())\n",
    "roi_sample = singleband_point_query(\n",
    "    fc=roi_sample,\n",
    "    img=slope,\n",
    "    scale=slope.projection().nominalScale().getInfo(),\n",
    "    prop='slope'\n",
    ")\n",
    "\n",
    "## aspect\n",
    "aspect = ee.Terrain.aspect(dem.mosaic())\n",
    "roi_sample = singleband_point_query(\n",
    "    fc=roi_sample,\n",
    "    img=aspect,\n",
    "    scale=aspect.projection().nominalScale().getInfo(),\n",
    "    prop='aspect'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLmXZm2aTihK"
   },
   "source": [
    "### Land cover\n",
    "In this cell, we lookup the different landcover classes.\n",
    "\n",
    "Dataset: [ESA WorldCover 10m v200](https://developers.google.com/earth-engine/datasets/catalog/ESA_WorldCover_v200?hl=en#description)\n",
    "\n",
    "> **Task:** How many different classes does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FXTCTeQotYs"
   },
   "outputs": [],
   "source": [
    "roi_sample = singleband_point_query(\n",
    "    fc=roi_sample,\n",
    "    img=landcover_ds.first(),\n",
    "    scale=landcover_ds.first().projection().nominalScale().getInfo(),\n",
    "    prop='landcover'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1E7DDNoDTry"
   },
   "source": [
    "### Flow accumulation\n",
    "Next, we are determining the upstream drainage area (flow accumulation area) for each of our sample locations. Again, we are just using our custom function for the lookup.\n",
    "\n",
    "Dataset: [MERIT Hydro: Global Hydrography Datasets](https://developers.google.com/earth-engine/datasets/catalog/MERIT_Hydro_v1_0_1#bands)\n",
    "\n",
    "> **Task:** Which unit should we expect for the upstream drainage area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uc54vThODWFJ"
   },
   "outputs": [],
   "source": [
    "roi_sample = singleband_point_query(\n",
    "    fc=roi_sample,\n",
    "    img=flowaccumulation,\n",
    "    scale=flowaccumulation.projection().nominalScale().getInfo(),\n",
    "    prop='upstream_drainage_area'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_Tx_7DyTU3A"
   },
   "source": [
    "### Precipitation\n",
    "The precipitation data is different from the other datasets as it does not represent a certain point in time but actually contains over 15'000 individual layers representing over 30 years of daily precipitation. To reduce the number of input variables for our model, we will aggregate the data and only take the maximum precipitation for each pixel and within each year between 2000 and 2018.\n",
    "\n",
    "Dataset: [CHIRPS Daily: Climate Hazards Group InfraRed Precipitation With Station Data (Version 2.0 Final)](https://developers.google.com/earth-engine/datasets/catalog/UCSB-CHG_CHIRPS_DAILY#description)\n",
    "\n",
    "> **Task:** Why do we not use the full dataset? Why do we limit ourselves to only the years 2000 - 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSVTlGCEnPf0"
   },
   "outputs": [],
   "source": [
    "for year in range(2000, 2019):\n",
    "  daily_max = precipitation_ds.select('precipitation').filter(ee.Filter.date(f\"{year}-01-01\", f\"{year+1}-01-01\")).max()\n",
    "  roi_sample = singleband_point_query(\n",
    "      fc=roi_sample,\n",
    "      img=daily_max,\n",
    "      scale=daily_max.projection().nominalScale().getInfo(),\n",
    "      prop=f\"daily_max_precipitation_{year}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ7hbVCYM76Y"
   },
   "source": [
    "### Historic flood\n",
    "The last information which is missing is the information if a location has been flooded in the past or not. This will be our target variable for the model. Again, we can just use are custom function to run the point lookup on the historic floods dataset.\n",
    "\n",
    "Dataset: [Global Flood Database v1 (2000-2018)](https://developers.google.com/earth-engine/datasets/catalog/GLOBAL_FLOOD_DB_MODIS_EVENTS_V1#description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VmfC2AuM21H"
   },
   "outputs": [],
   "source": [
    "roi_sample = singleband_point_query(\n",
    "    fc=roi_sample,\n",
    "    img=all_floods,\n",
    "    scale=flood_ds.select('flooded').first().projection().nominalScale().getInfo(),\n",
    "    prop='floods'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7m9QdNl9LEf"
   },
   "source": [
    "### Compute and export results\n",
    "As mentioned above, calling our custom point query-function has not triggered any computations but we merely defined the exact steps which need to be executed. We will now create a task to run all these steps on Google Earth Engine. The results will be stored on your Google Drive as a csv-table. This cell might run for a few minutes so we can use the time to remember (and appreciate) what is actually happening:\n",
    "1. run point lookup on the DEM,\n",
    "2. compute slope from the DEM and run point lookup,\n",
    "3. compute aspect from the DEM and run point lookup,\n",
    "4. run point lookup on the land cover dataset,\n",
    "5. run point lookup upstream drainage area dataset,\n",
    "6. aggregate the yearly maximum daily precipitation for two decades and run point lookup,\n",
    "7. run point lookup on the historic floods dataset,\n",
    "8. write the result to a .csv-file.\n",
    "\n",
    "The computation time is depending on the size of your region of interest but keeping in mind that some of the datasets we use have spatial resolutions of 30 (or even 10) meters, the speed of Google Earth Engine is pretty impressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOVtH-Ty_7o4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## remove output file if it already exists\n",
    "try:\n",
    "   (data_dir/'roi_sample.csv').unlink()\n",
    "except FileNotFoundError:\n",
    "  pass\n",
    "\n",
    "## create an export task\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection=roi_sample,\n",
    "    description='data export to Google Drive',\n",
    "    folder=data_dir.name,\n",
    "    fileNamePrefix='roi_sample',\n",
    "    fileFormat='CSV'\n",
    "    )\n",
    "\n",
    "## run and monitor export task\n",
    "task.start()\n",
    "while task.active():\n",
    "  sleep(10)\n",
    "task.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiQpDbBhM6EE"
   },
   "source": [
    "With the code below, we import the .csv file generated in GEE from Google Drive. We can now - finally - see the results of our data enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQDM7-jvK-Gy"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir/'roi_sample.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXX7B53pM8BJ"
   },
   "source": [
    "### Normalize elevation\n",
    "In this last post-processing step, we normalize the elevation values to a range between 0 and 1. For this, we get the minimum and maximum elevation within our ROI and apply the [formula for linear scaling](https://developers.google.com/machine-learning/data-prep/transform/normalization#scaling-to-a-range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsxkWc2XOcnP"
   },
   "outputs": [],
   "source": [
    "## determine min/max elevation within roi\n",
    "dem_range = dem.mosaic().reduceRegion(\n",
    "    geometry=roi.geometry(),\n",
    "    reducer=ee.Reducer.minMax(),\n",
    "    scale=dem.first().projection().nominalScale().getInfo(),\n",
    "    bestEffort=True\n",
    ").getInfo()\n",
    "\n",
    "\n",
    "## normalize elevation\n",
    "df['elevation'] = (df['elevation'] - dem_range.get('DEM_min')) / (dem_range.get('DEM_max') - dem_range.get('DEM_min'))\n",
    "df.to_csv(data_dir/'roi_sample_norm.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSA6PsDaHfrV"
   },
   "source": [
    "---\n",
    "## Repeating the same steps but for all 913 historic events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvYgQ2JURTsK"
   },
   "source": [
    "### Defining the training locations\n",
    "We sample the same number of locations as before but now, we not only sample within a single ROI but within the geometry of each and every event in our historic event dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6j5xMMMH0js"
   },
   "outputs": [],
   "source": [
    "## sample the same number of sample points for each flood event\n",
    "event_sample_size = SAMPLE_SIZE // flood_ds.size().getInfo()\n",
    "\n",
    "## randomly sample points within each event geometry\n",
    "def mapper(feat):\n",
    "  return ee.FeatureCollection.randomPoints(\n",
    "      region=feat.geometry(),\n",
    "      points=event_sample_size,\n",
    "      seed=SEED\n",
    "  )\n",
    "flood_sample = flood_ds.map(mapper).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntF65MA5R-wc"
   },
   "source": [
    "### Data enrichment\n",
    "Here, we can repeat exactly the same steps as before. The only difference is that we use a different set of training locations for the point lookups. Actually, it would be elegant to define not only the point lookup but the whole data enrichment pipeline as a single function but I deliberately left that out to keep the code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnYLVlWOMsXc"
   },
   "outputs": [],
   "source": [
    "## elevation\n",
    "flood_sample = singleband_point_query(\n",
    "    fc=flood_sample,\n",
    "    img=dem.mosaic(),\n",
    "    scale=dem.first().projection().nominalScale().getInfo(),\n",
    "    prop='elevation'\n",
    ")\n",
    "\n",
    "## slope\n",
    "flood_sample= singleband_point_query(\n",
    "    fc=flood_sample,\n",
    "    img=slope,\n",
    "    scale=slope.projection().nominalScale().getInfo(),\n",
    "    prop='slope'\n",
    ")\n",
    "\n",
    "## aspect\n",
    "flood_sample = singleband_point_query(\n",
    "    fc=flood_sample,\n",
    "    img=aspect,\n",
    "    scale=aspect.projection().nominalScale().getInfo(),\n",
    "    prop='aspect'\n",
    ")\n",
    "\n",
    "## land cover\n",
    "flood_sample = singleband_point_query(\n",
    "    fc=flood_sample,\n",
    "    img=landcover_ds.first(),\n",
    "    scale=landcover_ds.first().projection().nominalScale().getInfo(),\n",
    "    prop='landcover'\n",
    ")\n",
    "\n",
    "## flow accumulation\n",
    "flood_sample = singleband_point_query(\n",
    "    fc=flood_sample,\n",
    "    img=flowaccumulation,\n",
    "    scale=flowaccumulation.projection().nominalScale().getInfo(),\n",
    "    prop='upstream_drainage_area'\n",
    ")\n",
    "\n",
    "## precipitation\n",
    "for year in range(2000, 2019):\n",
    "  daily_max = precipitation_ds.select('precipitation').filter(ee.Filter.date(f\"{year}-01-01\", f\"{year+1}-01-01\")).max()\n",
    "  flood_sample = singleband_point_query(\n",
    "      fc=flood_sample,\n",
    "      img=daily_max,\n",
    "      scale=daily_max.projection().nominalScale().getInfo(),\n",
    "      prop=f\"daily_max_precipitation_{year}\"\n",
    "  )\n",
    "\n",
    "## historic flood\n",
    "flood_sample = singleband_point_query(\n",
    "    fc=flood_sample,\n",
    "    img=all_floods,\n",
    "    scale=flood_ds.select('flooded').first().projection().nominalScale().getInfo(),\n",
    "    prop='floods'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paXlSWdkTV-k"
   },
   "source": [
    "Same as above, we have to explicitly create a task to actually run the computations on GEE. This will take a bit longer as the last one but while you wait, you can have a look at some apps built with GEE:\n",
    "- https://www.earthengine.app/\n",
    "- https://philippgaertner.github.io/2020/12/ee-apps-table-searchable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ncUFRkBMsUS"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## remove output file if it already exists\n",
    "try:\n",
    "   (data_dir/'flood_sample.csv').unlink()\n",
    "except FileNotFoundError:\n",
    "  pass\n",
    "\n",
    "## create an export task\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection=flood_sample,\n",
    "    description=\"data export to Google Drive\",\n",
    "    folder=data_dir.name,\n",
    "    fileNamePrefix='flood_sample',\n",
    "    fileFormat='CSV'\n",
    "    )\n",
    "\n",
    "## run and monitor export task\n",
    "task.start()\n",
    "while task.active():\n",
    "  sleep(10)\n",
    "task.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9bfiFO4Uw1a"
   },
   "source": [
    "The normalization of elevation is a bit trickier than before because we do not want to normalize the full dataset but for each event separately.\n",
    "\n",
    "First, we want to exract all event geometries as individual polygons. This seems to be a rather hard task in GEE so we use [geopandas](https://geopandas.org/en/stable/) - an asbolute core library when working with vector data in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dP6eLXXZHnhD"
   },
   "outputs": [],
   "source": [
    "## get all image geometries as multipolygon\n",
    "multipolygon_fc = ee.FeatureCollection(flood_ds.select('flooded').geometry())\n",
    "\n",
    "## explode multipolygon to a list of polygons using geopandas\n",
    "gdf = geemap.ee_to_geopandas(multipolygon_fc)\n",
    "gdf.set_crs(4326, inplace=True)\n",
    "\n",
    "## create gee featurecollection of singlepolygons\n",
    "singlepolygons_fc = geemap.geopandas_to_ee(gdf.explode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUxmaQnBV6hX"
   },
   "source": [
    "Second, we use the dataset we created in the cell above to get the minimum and maximum elevation within each event geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vd0MsOgaxIwM"
   },
   "outputs": [],
   "source": [
    "## define some input variables\n",
    "dem = elevation_ds.select('DEM')\n",
    "dem_mosaic = dem.mosaic()\n",
    "scale = dem.first().projection().nominalScale().getInfo()\n",
    "\n",
    "## define the mapper\n",
    "def mapper(img):\n",
    "\n",
    "  ## get the minimum and maximum elevation within the event geometry\n",
    "  dem_range = dem_mosaic.reduceRegion(\n",
    "      geometry=img.geometry(),\n",
    "      reducer=ee.Reducer.minMax(),\n",
    "      scale=scale,\n",
    "      bestEffort=True\n",
    "  )\n",
    "\n",
    "  ## create a feature and assign the relevant properties\n",
    "  feat = ee.Feature(img.geometry())\n",
    "  feat = feat.set('id', img.get('id'))\n",
    "  feat = feat.set('system_index', img.get('system:index'))\n",
    "  feat = feat.set('elevation_min', dem_range.get('DEM_min'))\n",
    "  feat =feat.set('elevation_max', dem_range.get('DEM_max'))\n",
    "\n",
    "  return feat\n",
    "\n",
    "## map the mapper over all images in the historic flood dataset\n",
    "dem_ranges = ee.FeatureCollection(flood_ds.select('flooded').map(mapper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXfDT9jWW9VA"
   },
   "source": [
    "And again, we need to actually run the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rm4tqt0RNk4F"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## remove output file if it already exists\n",
    "try:\n",
    "   (data_dir/'dem_ranges.csv').unlink()\n",
    "except FileNotFoundError:\n",
    "  pass\n",
    "\n",
    "## create an export task\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection=dem_ranges,\n",
    "    description=\"data export to Google Drive\",\n",
    "    folder=data_dir.name,\n",
    "    fileNamePrefix='dem_ranges',\n",
    "    fileFormat='CSV'\n",
    "    )\n",
    "\n",
    "## run and monitor export task\n",
    "task.start()\n",
    "while task.active():\n",
    "  sleep(10)\n",
    "task.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yw41EFbEZIkd"
   },
   "source": [
    "Finally, we are importing the enriched training data and the dataset of min/max elevation so we can bring it together in order to create our final training dataset.\n",
    "\n",
    "> **Task:** Why do we need to normalize the elevation values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5c-55raf3z_"
   },
   "outputs": [],
   "source": [
    "## import data\n",
    "df = pd.read_csv(data_dir/'flood_sample.csv')\n",
    "ele = pd.read_csv(data_dir/'dem_ranges.csv')\n",
    "\n",
    "## add minimum and maximum elevation to training dataset\n",
    "df['system:index'] = df['system:index'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "df = df.merge(ele[['system:index', 'elevation_min', 'elevation_max']], how='left', on='system:index')\n",
    "\n",
    "## normalize elevation\n",
    "df['elevation'] = (df['elevation'] - df['elevation'].min()) / (df['elevation'].max() - df['elevation'].min())\n",
    "\n",
    "## drop locations where any attribute is NaN\n",
    "df = df[~df.isnull().any(axis=1)]\n",
    "\n",
    "## write dataframe to csv\n",
    "df.to_csv(data_dir/'flood_sample_norm.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjDC_B90cXpd"
   },
   "source": [
    "### Exploring the training dataset\n",
    "> **Task:** Run the remaining two cells and have a look at their output. Try to answer the following questions:\n",
    "1.  Is our training dataset biased? Try to think about the spatial and temporal limitations we mentioned when looking at the datasets.\n",
    "2. How would you rate the distribution of our training data with respect to the *flooded*-attribute? Is it optimal for our purpose?\n",
    "3. Where would you see room for improvement for the sampling approach? How would you implement your idea if you had two write code? No need to get too detailed but I am interested in your approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qe5hJmPvcgnl"
   },
   "outputs": [],
   "source": [
    "## print flood counts\n",
    "for k,v in df['floods'].value_counts().to_dict().items():\n",
    "  print(f\"locations with {int(k)} floods: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rqH3Q1hc1ff"
   },
   "outputs": [],
   "source": [
    "## add sampled locations to the map of historic floods\n",
    "Map.add_layer(flood_sample, None, 'Training dataset')\n",
    "Map"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
